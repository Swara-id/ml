{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import mediapipe as mp\n",
    "\n",
    "import numpy as np \n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "\n",
    "from apscheduler.schedulers.background import BackgroundScheduler\n",
    "from apscheduler.executors.pool import ThreadPoolExecutor\n",
    "import sched\n",
    "import threading\n",
    "from difflib import get_close_matches\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class labels\n",
    "class_labels = ['A', 'Apa', 'B', 'Baik', 'Berapa', 'C', 'D', 'Dimana', 'E', 'F',\n",
    "       'G', 'H', 'Halo', 'I', 'J', 'K', 'Kapan', 'Kemana', 'L', 'M',\n",
    "       'Mengapa', 'N', 'O', 'P', 'Q', 'R', 'S', 'Sabar', 'Siapa',\n",
    "       'T', 'Tidur', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
    "\n",
    "# Load the TFLite model with preprocessing\n",
    "interpreter = tf.lite.Interpreter(model_path=\"./model_with_preprocessing.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to extract hand features from an image\n",
    "def extract_hand_features_and_draw(image):\n",
    "    mp_hands = mp.solutions.hands\n",
    "    hands = mp_hands.Hands(\n",
    "        static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5\n",
    "    )\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(image_rgb)\n",
    "    data = []\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            landmarks = []\n",
    "            for landmark in hand_landmarks.landmark:\n",
    "                landmarks.append(landmark.x)\n",
    "                landmarks.append(landmark.y)\n",
    "                landmarks.append(landmark.z)\n",
    "            data.append(landmarks) \n",
    "        \n",
    "        mp.solutions.drawing_utils.draw_landmarks(\n",
    "            image, hand_landmarks, mp_hands.HAND_CONNECTIONS\n",
    "        )\n",
    "\n",
    "    hands.close()\n",
    "    if len(data) == 2:\n",
    "        # Flatten the list and concatenate features from both hands\n",
    "        features = np.concatenate((data[0], data[1]))\n",
    "    elif len(data) == 1:\n",
    "        # Duplicate the single hand's features to simulate two-hand input\n",
    "        features = np.concatenate((data[0], data[0]))\n",
    "    else:\n",
    "        # Return an empty array if no hands are detected\n",
    "        features = np.array([])\n",
    "\n",
    "    return features, image\n",
    "\n",
    "\n",
    "# Function to run inference on an image\n",
    "def run_inference(hand_features):\n",
    "    if hand_features.size == 0:\n",
    "        return None\n",
    "\n",
    "    hand_features = hand_features.reshape(1, -1).astype(np.float32)\n",
    "\n",
    "    interpreter.set_tensor(input_details[0][\"index\"], hand_features)\n",
    "    interpreter.invoke()\n",
    "    output_data = interpreter.get_tensor(output_details[0][\"index\"])\n",
    "    predicted_class_index = np.argmax(output_data)\n",
    "    return predicted_class_index\n",
    "\n",
    "def generate_word_spelling(file_path, del_thresh):\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    filtered_df = df[df[0].str.len() >= del_thresh]\n",
    "    words = filtered_df[0].tolist()\n",
    "    return words\n",
    "\n",
    "def correct_spelling(word, word_list):\n",
    "    same_length_words = [w for w in word_list if len(w) == len(word)]\n",
    "    close_matches = get_close_matches(word, same_length_words, n=1, cutoff=0.8)\n",
    "    \n",
    "    print(close_matches)\n",
    "    if close_matches:\n",
    "        return close_matches[0]\n",
    "    \n",
    "    for i in range(len(word)-1, -1, -1):\n",
    "        for char in 'abcdefghijklmnopqrstuvwxyz':\n",
    "            if char != word[i]:  # Hanya mengganti jika karakter berbeda\n",
    "                possible_word = word[:i] + char + word[i+1:]\n",
    "                if possible_word in word_list:\n",
    "                    return possible_word\n",
    "    \n",
    "    for char in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        possible_word = word + char\n",
    "        if possible_word in word_list:\n",
    "            return possible_word\n",
    "    \n",
    "    print(word)\n",
    "    return word\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yang', 'dan', 'ini', 'untuk', 'mereka']\n",
      "10651\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'bagaimana'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_word = generate_word_spelling('./10k-indonesia-common-words.csv', 3)\n",
    "print(list_word[:5])\n",
    "print(len(list_word))\n",
    "correct_spelling('bagaimans', list_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n",
      "Aku makan ayam bersama ibu\n"
     ]
    }
   ],
   "source": [
    "def gabung_kalimat(kata_list):\n",
    "    if len(kata_list) > 0:\n",
    "        print('true')\n",
    "        kata_list_cleaned = []\n",
    "        for kata in kata_list:\n",
    "            kata_list_cleaned.append(correct_spelling(kata, list_word))\n",
    "\n",
    "        # kalimat = check_and_correct(kata_list[0])\n",
    "        kalimat = kata_list_cleaned[0]\n",
    "        \n",
    "        for i in range(1, len(kata_list_cleaned)):\n",
    "            kata = kata_list_cleaned[i]\n",
    "            # kata = check_and_correct(kata_list_cleaned[i])\n",
    "            prev_kata = kata_list_cleaned[i-1]\n",
    "\n",
    "            if len(prev_kata) > 1 or (len(prev_kata) == 1 and len(kata) > 1):\n",
    "                kalimat += \" \" + kata\n",
    "            else:\n",
    "                kalimat += kata\n",
    "        \n",
    "        return kalimat.capitalize()\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "hasil_gabung = gabung_kalimat(['aks', 'makan', 'aygm', 'b', 'e', 'r', 's', 'a', 'm', 'a', 'ibu'])\n",
    "# hasil_gabung = gabung_kalimat([])\n",
    "print(hasil_gabung)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables for sentence prediction\n",
    "sentence = \"\"\n",
    "last_detected_time = time.time()\n",
    "last_prediction_time = time.time()\n",
    "reset_time = 5  # seconds\n",
    "prediction_delay = 1  # seconds\n",
    "movement_threshold = 0.02  # Threshold for hand movement change\n",
    "last_prediction = None\n",
    "last_hand_features = None\n",
    "hand_detected = False\n",
    "detection_time = 0\n",
    "\n",
    "# Start video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    current_time = time.time()\n",
    "\n",
    "    if not hand_detected:\n",
    "        hand_features, frame_with_landmarks = extract_hand_features_and_draw(frame)\n",
    "        if hand_features.size != 0:\n",
    "            hand_detected = True\n",
    "            detection_time = current_time\n",
    "\n",
    "    # Wait for 400ms after hand detection\n",
    "    if (\n",
    "        hand_detected\n",
    "        and (current_time - detection_time >= prediction_delay)\n",
    "        and (current_time - last_prediction_time >= prediction_delay)\n",
    "    ):\n",
    "        hand_features, frame_with_landmarks = extract_hand_features_and_draw(frame)\n",
    "        prediction = run_inference(hand_features)\n",
    "        last_prediction_time = current_time\n",
    "\n",
    "        if prediction is not None:\n",
    "            # Check hand movement change\n",
    "            if last_hand_features is not None:\n",
    "                movement_change = np.linalg.norm(hand_features - last_hand_features)\n",
    "                if movement_change < movement_threshold:\n",
    "                    hand_detected = False\n",
    "                    continue\n",
    "\n",
    "            # Reset timer since we detected a hand\n",
    "            last_detected_time = current_time\n",
    "            predicted_label = class_labels[prediction]\n",
    "\n",
    "            if (\n",
    "                last_prediction is not None\n",
    "                and len(predicted_label) == 1\n",
    "                and len(last_prediction) == 1\n",
    "            ):\n",
    "                sentence += predicted_label\n",
    "            else:\n",
    "                # if the last predicted label is a single character,\n",
    "                # we need to use correct_spelling to correct the spelling\n",
    "                if last_prediction is not None and len(last_prediction) == 1:\n",
    "                    print(\"masuk\", last_prediction)\n",
    "                    sentence = correct_spelling(sentence, list_word)\n",
    "                    print(sentence)\n",
    "                sentence += \" \" + predicted_label\n",
    "            last_prediction = predicted_label\n",
    "\n",
    "            last_hand_features = hand_features\n",
    "        hand_detected = False\n",
    "\n",
    "    else:\n",
    "        # Check if the reset time has elapsed\n",
    "        if current_time - last_detected_time > reset_time:\n",
    "            sentence = \"\"\n",
    "            last_prediction = None\n",
    "            last_hand_features = None\n",
    "\n",
    "    # Display the current sentence\n",
    "    cv2.putText(\n",
    "        frame,\n",
    "        f\"Sentence: {sentence}\",\n",
    "        (10, 30),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        1,\n",
    "        (255, 0, 0),\n",
    "        2,\n",
    "        cv2.LINE_AA,\n",
    "    )\n",
    "    cv2.imshow(\"Real-time Sentence Prediction\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
